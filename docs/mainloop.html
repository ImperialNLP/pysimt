<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysimt.mainloop API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysimt.mainloop</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
import time
import logging

import torch

from .evaluator import Evaluator
from .optimizer import Optimizer
from .monitor import Monitor
from .utils.nn import get_module_groups
from .utils.misc import load_pt_file
from .utils.ml_metrics import Loss
from .utils.data import make_dataloader
from .utils.tensorboard import TensorBoard
from .translators import get_translator

logger = logging.getLogger(&#39;pysimt&#39;)


class MainLoop:
    def __init__(self, model, train_opts, dev_mgr):
        # Get all training options into this mainloop
        self.__dict__.update(train_opts)

        self.print = logger.info
        self.model = model
        self.dev_mgr = dev_mgr
        self.epoch_valid = (self.eval_freq == 0)
        self.oom_count = 0
        self.loss_meter = Loss()
        self._found_optim_state = None

        # Load training and validation data &amp; create iterators
        self.print(&#39;Loading dataset(s)&#39;)
        self.train_iterator = make_dataloader(
            self.model.load_data(&#39;train&#39;, self.batch_size),
            self.pin_memory, self.num_workers)

        # Create monitor for validation, evaluation, checkpointing stuff
        self.monitor = Monitor(self.save_path / self.subfolder, self.exp_id,
                               self.model, logger, self.patience,
                               self.eval_metrics,
                               save_best_metrics=self.save_best_metrics,
                               n_checkpoints=self.n_checkpoints)

        # If a validation set exists
        if &#39;val_set&#39; in self.model.opts.data and self.eval_freq &gt;= 0:
            if &#39;LOSS&#39; in self.monitor.eval_metrics:
                self.vloss_iterator = make_dataloader(
                    self.model.load_data(&#39;val&#39;, self.batch_size, mode=&#39;eval&#39;))

            if self.monitor.beam_metrics is not None:
                self.beam_iterator = make_dataloader(
                    self.model.load_data(&#39;val&#39;, self.eval_batch_size, mode=&#39;beam&#39;))
                # Create hypothesis evaluator
                self.evaluator = Evaluator(
                    self.model.val_refs, self.monitor.beam_metrics,
                    filters=self.eval_filters)
                self.translator = get_translator(self.model.opts.model[&#39;translator_type&#39;])(
                    self.model, self.beam_iterator, out_prefix=&#39;&#39;,
                    batch_size=self.eval_batch_size,
                    max_len=self.eval_max_len)

        # Setup model
        self.model.setup()
        self.model.reset_parameters()

        ################################################
        # Initialize model weights with a pretrained one
        # This should come after model.setup()
        ################################################
        if train_opts[&#39;pretrained_file&#39;]:
            # Relax the strict condition for partial initialization
            data = load_pt_file(train_opts[&#39;pretrained_file&#39;])
            weights = data[&#39;model&#39;]
            self._found_optim_state = data.get(&#39;optimizer&#39;, None)
            if train_opts[&#39;pretrained_layers&#39;]:
                prefixes = tuple(train_opts[&#39;pretrained_layers&#39;].split(&#39;,&#39;))
                keys = [w for w in weights if w.startswith(prefixes)]
                weights = {k: weights[k] for k in keys}

            for name in get_module_groups(weights.keys()):
                self.print(
                    &#39; -&gt; will initialize {}.* with pretrained weights.&#39;.format(name))
            model.load_state_dict(weights, strict=False)

        ############################
        # Freeze layers if requested
        ############################
        if train_opts[&#39;freeze_layers&#39;]:
            frozen = []
            for layer in train_opts[&#39;freeze_layers&#39;].split(&#39;,&#39;):
                for name, param in self.model.named_parameters():
                    if name.startswith(layer):
                        param.requires_grad = False
                        frozen.append(name)

            for name in frozen:
                self.print(f&#39; -&gt; froze parameter {name}&#39;)

        self.print(self.model)
        self.model = self.model.to(self.dev_mgr.dev)

        if self.dev_mgr.req_cpu or len(self.dev_mgr.cuda_dev_ids) == 1:
            self.net = self.model
        else:
            self.net = torch.nn.DataParallel(
                self.model, device_ids=self.dev_mgr.cuda_dev_ids, dim=1)

        # Create optimizer instance
        self.optim = Optimizer(
            self.optimizer, self.model, lr=self.lr, momentum=self.momentum,
            nesterov=self.nesterov, weight_decay=self.l2_reg,
            gclip=self.gclip, lr_decay=self.lr_decay,
            lr_decay_factor=self.lr_decay_factor,
            lr_decay_mode=self.monitor.lr_decay_mode,
            lr_decay_min=self.lr_decay_min,
            lr_decay_patience=self.lr_decay_patience,
            tf_model_dim=self.tf_model_dim,
            lr_warmup_steps=self.lr_warmup_steps,
            adam_betas=self.adam_betas,
        )
        self.print(self.optim)

        if self._found_optim_state:
            # NOTE: This will overwrite weight_decay and lr parameters
            # from the checkpoint without obeying to new config file!
            self.optim.load_state_dict(self._found_optim_state)

        if self.save_optim_state:
            self.monitor.set_optimizer(self.optim)

        # Create TensorBoard logger if possible and requested
        self.tboard = TensorBoard(self.model, self.tensorboard_dir,
                                  self.exp_id, self.subfolder)
        self.print(self.tboard)

        # Models can also use tensorboard for custom purposes
        self.model.register_tensorboard(self.tboard)

    def train_batch(self, batch):
        &#34;&#34;&#34;Trains a batch.&#34;&#34;&#34;
        nn_start = time.time()

        # Reset gradients
        self.optim.zero_grad()

        # Forward pass with training progress
        # NOTE: Problematic for multi-gpu
        out = self.net(batch, uctr=self.monitor.uctr, ectr=self.monitor.ectr)
        self.loss_meter.update(out[&#39;loss&#39;], out[&#39;n_items&#39;])
        loss = out[&#39;loss&#39;] / out[&#39;n_items&#39;]

        # Add other losses if any
        if self.net.aux_loss:
            loss += sum(list(self.net.aux_loss.values()))

        # Backward pass
        loss.backward()

        # Update parameters (includes gradient clipping logic)
        self.optim.step()

        return time.time() - nn_start

    def train_epoch(self):
        &#34;&#34;&#34;Trains a full epoch.&#34;&#34;&#34;
        self.print(&#39;Starting Epoch {}&#39;.format(self.monitor.ectr))

        nn_sec = 0.0
        eval_sec = 0.0
        total_sec = time.time()
        self.loss_meter.reset()
        self.oom_count = 0

        for batch in self.train_iterator:
            batch.device(self.dev_mgr.dev)
            self.monitor.uctr += 1

            try:
                nn_sec += self.train_batch(batch)
            except RuntimeError as e:
                if self.handle_oom and &#39;out of memory&#39; in e.args[0]:
                    torch.cuda.empty_cache()
                    self.oom_count += 1
                else:
                    raise e

            if self.monitor.uctr % self.disp_freq == 0:
                # Send statistics
                self.tboard.log_scalar(
                    &#39;train_LOSS&#39;, self.loss_meter.batch_loss, self.monitor.uctr)

                msg = &#34;Epoch {} - update {:10d} =&gt; loss: {:&gt;7.3f}&#34;.format(
                    self.monitor.ectr, self.monitor.uctr,
                    self.loss_meter.batch_loss)
                for key, value in self.net.aux_loss.items():
                    val = value.item()
                    msg += &#39; [{}: {:.3f}]&#39;.format(key, val)
                    self.tboard.log_scalar(&#39;train_&#39; + key.upper(), val, self.monitor.uctr)
                msg += &#39; (#OOM: {})&#39;.format(self.oom_count)
                self.print(msg)

            # Do validation?
            if (not self.epoch_valid and
                    self.monitor.ectr &gt;= self.eval_start and
                    self.eval_freq &gt; 0 and
                    self.monitor.uctr % self.eval_freq == 0):
                eval_start = time.time()
                self.do_validation()
                eval_sec += time.time() - eval_start

            if (self.checkpoint_freq and self.n_checkpoints &gt; 0 and
                    self.monitor.uctr % self.checkpoint_freq == 0):
                self.print(&#39;Saving checkpoint...&#39;)
                self.monitor.save_checkpoint()

            # Check stopping conditions
            if self.monitor.early_bad == self.monitor.patience:
                self.print(&#34;Early stopped.&#34;)
                return False

            if self.monitor.uctr == self.max_iterations:
                self.print(&#34;Max iterations {} reached.&#34;.format(
                    self.max_iterations))
                return False

        # All time spent for this epoch
        total_min = (time.time() - total_sec) / 60
        # All time spent during forward/backward/step
        nn_min = nn_sec / 60
        # All time spent during validation(s)
        eval_min = eval_sec / 60
        # Rest is iteration overhead + checkpoint saving
        overhead_min = total_min - nn_min - eval_min

        # Compute epoch loss
        epoch_loss = self.loss_meter.get()
        self.monitor.train_loss.append(epoch_loss)

        self.print(&#34;--&gt; Epoch {} finished with mean loss {:.5f}&#34;.format(
            self.monitor.ectr, epoch_loss))
        self.print(&#34;--&gt; Overhead/Training/Evaluation: {:.2f}/{:.2f}/{:.2f} &#34;
                   &#34;mins (total: {:.2f} mins)   ({} samples/sec)&#34;.format(
                       overhead_min, nn_min, eval_min, total_min,
                       int(len(self.train_iterator.dataset) / nn_sec)))

        # Do validation?
        if self.epoch_valid and self.monitor.ectr &gt;= self.eval_start:
            self.do_validation()

        # Check whether maximum epoch is reached
        if self.monitor.ectr == self.max_epochs:
            self.print(&#34;Max epochs {} reached.&#34;.format(self.max_epochs))
            return False

        self.monitor.ectr += 1
        return True

    def do_validation(self):
        &#34;&#34;&#34;Do early-stopping validation.&#34;&#34;&#34;
        results = []
        self.monitor.vctr += 1
        self.net.train(False)
        torch.set_grad_enabled(False)

        # Collect simple validation stats first
        self.print(&#39;Computing evaluation loss...&#39;)
        results.extend(self.net.test_performance(self.vloss_iterator))

        if self.monitor.beam_metrics:
            tr_args = self.model.opts.model.get(&#39;translator_args&#39;, {})
            self.print(f&#39;Performing greedy search (args: {tr_args})&#39;)
            beam_time = time.time()
            # Use greedy search
            hyps, *_ = self.translator.run(**tr_args)
            beam_time = time.time() - beam_time

            # Compute metrics and update results
            score_time = time.time()
            results.extend(self.evaluator.score(hyps))
            score_time = time.time() - score_time

        # Log metrics to tensorboard
        self.tboard.log_metrics(results, self.monitor.uctr, suffix=&#39;val_&#39;)

        # Add new scores to history
        self.monitor.update_scores(results)

        # Do a scheduler LR step
        lr_change = self.optim.lr_step(self.monitor.get_last_eval_score())
        if lr_change and self.lr_decay_revert:
            self.print(&#39;Reloading previous best model parameters&#39;)
            self.monitor.reload_previous_best()

        # Check early-stop criteria and save snapshots if any
        self.monitor.save_models()

        # Dump summary and switch back to training mode
        self.monitor.val_summary()
        self.net.train(True)
        torch.set_grad_enabled(True)

    def __call__(self):
        &#34;&#34;&#34;Runs training loop.&#34;&#34;&#34;
        self.print(&#39;Training started on %s&#39; % time.strftime(&#39;%d-%m-%Y %H:%M:%S&#39;))
        self.net.train(True)
        torch.set_grad_enabled(True)

        # Evaluate once before even starting training
        if self.eval_zero:
            self.do_validation()

        while self.train_epoch():
            pass

        if self.monitor.vctr &gt; 0:
            self.monitor.val_summary()
        else:
            # No validation done, save final model
            self.print(&#39;Saving final model.&#39;)
            self.monitor.save_model(suffix=&#39;final&#39;)

        self.print(&#39;Training finished on %s&#39; % time.strftime(&#39;%d-%m-%Y %H:%M&#39;))
        # Close tensorboard
        self.tboard.close()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysimt.mainloop.MainLoop"><code class="flex name class">
<span>class <span class="ident">MainLoop</span></span>
<span>(</span><span>model, train_opts, dev_mgr)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MainLoop:
    def __init__(self, model, train_opts, dev_mgr):
        # Get all training options into this mainloop
        self.__dict__.update(train_opts)

        self.print = logger.info
        self.model = model
        self.dev_mgr = dev_mgr
        self.epoch_valid = (self.eval_freq == 0)
        self.oom_count = 0
        self.loss_meter = Loss()
        self._found_optim_state = None

        # Load training and validation data &amp; create iterators
        self.print(&#39;Loading dataset(s)&#39;)
        self.train_iterator = make_dataloader(
            self.model.load_data(&#39;train&#39;, self.batch_size),
            self.pin_memory, self.num_workers)

        # Create monitor for validation, evaluation, checkpointing stuff
        self.monitor = Monitor(self.save_path / self.subfolder, self.exp_id,
                               self.model, logger, self.patience,
                               self.eval_metrics,
                               save_best_metrics=self.save_best_metrics,
                               n_checkpoints=self.n_checkpoints)

        # If a validation set exists
        if &#39;val_set&#39; in self.model.opts.data and self.eval_freq &gt;= 0:
            if &#39;LOSS&#39; in self.monitor.eval_metrics:
                self.vloss_iterator = make_dataloader(
                    self.model.load_data(&#39;val&#39;, self.batch_size, mode=&#39;eval&#39;))

            if self.monitor.beam_metrics is not None:
                self.beam_iterator = make_dataloader(
                    self.model.load_data(&#39;val&#39;, self.eval_batch_size, mode=&#39;beam&#39;))
                # Create hypothesis evaluator
                self.evaluator = Evaluator(
                    self.model.val_refs, self.monitor.beam_metrics,
                    filters=self.eval_filters)
                self.translator = get_translator(self.model.opts.model[&#39;translator_type&#39;])(
                    self.model, self.beam_iterator, out_prefix=&#39;&#39;,
                    batch_size=self.eval_batch_size,
                    max_len=self.eval_max_len)

        # Setup model
        self.model.setup()
        self.model.reset_parameters()

        ################################################
        # Initialize model weights with a pretrained one
        # This should come after model.setup()
        ################################################
        if train_opts[&#39;pretrained_file&#39;]:
            # Relax the strict condition for partial initialization
            data = load_pt_file(train_opts[&#39;pretrained_file&#39;])
            weights = data[&#39;model&#39;]
            self._found_optim_state = data.get(&#39;optimizer&#39;, None)
            if train_opts[&#39;pretrained_layers&#39;]:
                prefixes = tuple(train_opts[&#39;pretrained_layers&#39;].split(&#39;,&#39;))
                keys = [w for w in weights if w.startswith(prefixes)]
                weights = {k: weights[k] for k in keys}

            for name in get_module_groups(weights.keys()):
                self.print(
                    &#39; -&gt; will initialize {}.* with pretrained weights.&#39;.format(name))
            model.load_state_dict(weights, strict=False)

        ############################
        # Freeze layers if requested
        ############################
        if train_opts[&#39;freeze_layers&#39;]:
            frozen = []
            for layer in train_opts[&#39;freeze_layers&#39;].split(&#39;,&#39;):
                for name, param in self.model.named_parameters():
                    if name.startswith(layer):
                        param.requires_grad = False
                        frozen.append(name)

            for name in frozen:
                self.print(f&#39; -&gt; froze parameter {name}&#39;)

        self.print(self.model)
        self.model = self.model.to(self.dev_mgr.dev)

        if self.dev_mgr.req_cpu or len(self.dev_mgr.cuda_dev_ids) == 1:
            self.net = self.model
        else:
            self.net = torch.nn.DataParallel(
                self.model, device_ids=self.dev_mgr.cuda_dev_ids, dim=1)

        # Create optimizer instance
        self.optim = Optimizer(
            self.optimizer, self.model, lr=self.lr, momentum=self.momentum,
            nesterov=self.nesterov, weight_decay=self.l2_reg,
            gclip=self.gclip, lr_decay=self.lr_decay,
            lr_decay_factor=self.lr_decay_factor,
            lr_decay_mode=self.monitor.lr_decay_mode,
            lr_decay_min=self.lr_decay_min,
            lr_decay_patience=self.lr_decay_patience,
            tf_model_dim=self.tf_model_dim,
            lr_warmup_steps=self.lr_warmup_steps,
            adam_betas=self.adam_betas,
        )
        self.print(self.optim)

        if self._found_optim_state:
            # NOTE: This will overwrite weight_decay and lr parameters
            # from the checkpoint without obeying to new config file!
            self.optim.load_state_dict(self._found_optim_state)

        if self.save_optim_state:
            self.monitor.set_optimizer(self.optim)

        # Create TensorBoard logger if possible and requested
        self.tboard = TensorBoard(self.model, self.tensorboard_dir,
                                  self.exp_id, self.subfolder)
        self.print(self.tboard)

        # Models can also use tensorboard for custom purposes
        self.model.register_tensorboard(self.tboard)

    def train_batch(self, batch):
        &#34;&#34;&#34;Trains a batch.&#34;&#34;&#34;
        nn_start = time.time()

        # Reset gradients
        self.optim.zero_grad()

        # Forward pass with training progress
        # NOTE: Problematic for multi-gpu
        out = self.net(batch, uctr=self.monitor.uctr, ectr=self.monitor.ectr)
        self.loss_meter.update(out[&#39;loss&#39;], out[&#39;n_items&#39;])
        loss = out[&#39;loss&#39;] / out[&#39;n_items&#39;]

        # Add other losses if any
        if self.net.aux_loss:
            loss += sum(list(self.net.aux_loss.values()))

        # Backward pass
        loss.backward()

        # Update parameters (includes gradient clipping logic)
        self.optim.step()

        return time.time() - nn_start

    def train_epoch(self):
        &#34;&#34;&#34;Trains a full epoch.&#34;&#34;&#34;
        self.print(&#39;Starting Epoch {}&#39;.format(self.monitor.ectr))

        nn_sec = 0.0
        eval_sec = 0.0
        total_sec = time.time()
        self.loss_meter.reset()
        self.oom_count = 0

        for batch in self.train_iterator:
            batch.device(self.dev_mgr.dev)
            self.monitor.uctr += 1

            try:
                nn_sec += self.train_batch(batch)
            except RuntimeError as e:
                if self.handle_oom and &#39;out of memory&#39; in e.args[0]:
                    torch.cuda.empty_cache()
                    self.oom_count += 1
                else:
                    raise e

            if self.monitor.uctr % self.disp_freq == 0:
                # Send statistics
                self.tboard.log_scalar(
                    &#39;train_LOSS&#39;, self.loss_meter.batch_loss, self.monitor.uctr)

                msg = &#34;Epoch {} - update {:10d} =&gt; loss: {:&gt;7.3f}&#34;.format(
                    self.monitor.ectr, self.monitor.uctr,
                    self.loss_meter.batch_loss)
                for key, value in self.net.aux_loss.items():
                    val = value.item()
                    msg += &#39; [{}: {:.3f}]&#39;.format(key, val)
                    self.tboard.log_scalar(&#39;train_&#39; + key.upper(), val, self.monitor.uctr)
                msg += &#39; (#OOM: {})&#39;.format(self.oom_count)
                self.print(msg)

            # Do validation?
            if (not self.epoch_valid and
                    self.monitor.ectr &gt;= self.eval_start and
                    self.eval_freq &gt; 0 and
                    self.monitor.uctr % self.eval_freq == 0):
                eval_start = time.time()
                self.do_validation()
                eval_sec += time.time() - eval_start

            if (self.checkpoint_freq and self.n_checkpoints &gt; 0 and
                    self.monitor.uctr % self.checkpoint_freq == 0):
                self.print(&#39;Saving checkpoint...&#39;)
                self.monitor.save_checkpoint()

            # Check stopping conditions
            if self.monitor.early_bad == self.monitor.patience:
                self.print(&#34;Early stopped.&#34;)
                return False

            if self.monitor.uctr == self.max_iterations:
                self.print(&#34;Max iterations {} reached.&#34;.format(
                    self.max_iterations))
                return False

        # All time spent for this epoch
        total_min = (time.time() - total_sec) / 60
        # All time spent during forward/backward/step
        nn_min = nn_sec / 60
        # All time spent during validation(s)
        eval_min = eval_sec / 60
        # Rest is iteration overhead + checkpoint saving
        overhead_min = total_min - nn_min - eval_min

        # Compute epoch loss
        epoch_loss = self.loss_meter.get()
        self.monitor.train_loss.append(epoch_loss)

        self.print(&#34;--&gt; Epoch {} finished with mean loss {:.5f}&#34;.format(
            self.monitor.ectr, epoch_loss))
        self.print(&#34;--&gt; Overhead/Training/Evaluation: {:.2f}/{:.2f}/{:.2f} &#34;
                   &#34;mins (total: {:.2f} mins)   ({} samples/sec)&#34;.format(
                       overhead_min, nn_min, eval_min, total_min,
                       int(len(self.train_iterator.dataset) / nn_sec)))

        # Do validation?
        if self.epoch_valid and self.monitor.ectr &gt;= self.eval_start:
            self.do_validation()

        # Check whether maximum epoch is reached
        if self.monitor.ectr == self.max_epochs:
            self.print(&#34;Max epochs {} reached.&#34;.format(self.max_epochs))
            return False

        self.monitor.ectr += 1
        return True

    def do_validation(self):
        &#34;&#34;&#34;Do early-stopping validation.&#34;&#34;&#34;
        results = []
        self.monitor.vctr += 1
        self.net.train(False)
        torch.set_grad_enabled(False)

        # Collect simple validation stats first
        self.print(&#39;Computing evaluation loss...&#39;)
        results.extend(self.net.test_performance(self.vloss_iterator))

        if self.monitor.beam_metrics:
            tr_args = self.model.opts.model.get(&#39;translator_args&#39;, {})
            self.print(f&#39;Performing greedy search (args: {tr_args})&#39;)
            beam_time = time.time()
            # Use greedy search
            hyps, *_ = self.translator.run(**tr_args)
            beam_time = time.time() - beam_time

            # Compute metrics and update results
            score_time = time.time()
            results.extend(self.evaluator.score(hyps))
            score_time = time.time() - score_time

        # Log metrics to tensorboard
        self.tboard.log_metrics(results, self.monitor.uctr, suffix=&#39;val_&#39;)

        # Add new scores to history
        self.monitor.update_scores(results)

        # Do a scheduler LR step
        lr_change = self.optim.lr_step(self.monitor.get_last_eval_score())
        if lr_change and self.lr_decay_revert:
            self.print(&#39;Reloading previous best model parameters&#39;)
            self.monitor.reload_previous_best()

        # Check early-stop criteria and save snapshots if any
        self.monitor.save_models()

        # Dump summary and switch back to training mode
        self.monitor.val_summary()
        self.net.train(True)
        torch.set_grad_enabled(True)

    def __call__(self):
        &#34;&#34;&#34;Runs training loop.&#34;&#34;&#34;
        self.print(&#39;Training started on %s&#39; % time.strftime(&#39;%d-%m-%Y %H:%M:%S&#39;))
        self.net.train(True)
        torch.set_grad_enabled(True)

        # Evaluate once before even starting training
        if self.eval_zero:
            self.do_validation()

        while self.train_epoch():
            pass

        if self.monitor.vctr &gt; 0:
            self.monitor.val_summary()
        else:
            # No validation done, save final model
            self.print(&#39;Saving final model.&#39;)
            self.monitor.save_model(suffix=&#39;final&#39;)

        self.print(&#39;Training finished on %s&#39; % time.strftime(&#39;%d-%m-%Y %H:%M&#39;))
        # Close tensorboard
        self.tboard.close()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysimt.mainloop.MainLoop.do_validation"><code class="name flex">
<span>def <span class="ident">do_validation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Do early-stopping validation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def do_validation(self):
    &#34;&#34;&#34;Do early-stopping validation.&#34;&#34;&#34;
    results = []
    self.monitor.vctr += 1
    self.net.train(False)
    torch.set_grad_enabled(False)

    # Collect simple validation stats first
    self.print(&#39;Computing evaluation loss...&#39;)
    results.extend(self.net.test_performance(self.vloss_iterator))

    if self.monitor.beam_metrics:
        tr_args = self.model.opts.model.get(&#39;translator_args&#39;, {})
        self.print(f&#39;Performing greedy search (args: {tr_args})&#39;)
        beam_time = time.time()
        # Use greedy search
        hyps, *_ = self.translator.run(**tr_args)
        beam_time = time.time() - beam_time

        # Compute metrics and update results
        score_time = time.time()
        results.extend(self.evaluator.score(hyps))
        score_time = time.time() - score_time

    # Log metrics to tensorboard
    self.tboard.log_metrics(results, self.monitor.uctr, suffix=&#39;val_&#39;)

    # Add new scores to history
    self.monitor.update_scores(results)

    # Do a scheduler LR step
    lr_change = self.optim.lr_step(self.monitor.get_last_eval_score())
    if lr_change and self.lr_decay_revert:
        self.print(&#39;Reloading previous best model parameters&#39;)
        self.monitor.reload_previous_best()

    # Check early-stop criteria and save snapshots if any
    self.monitor.save_models()

    # Dump summary and switch back to training mode
    self.monitor.val_summary()
    self.net.train(True)
    torch.set_grad_enabled(True)</code></pre>
</details>
</dd>
<dt id="pysimt.mainloop.MainLoop.train_batch"><code class="name flex">
<span>def <span class="ident">train_batch</span></span>(<span>self, batch)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains a batch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_batch(self, batch):
    &#34;&#34;&#34;Trains a batch.&#34;&#34;&#34;
    nn_start = time.time()

    # Reset gradients
    self.optim.zero_grad()

    # Forward pass with training progress
    # NOTE: Problematic for multi-gpu
    out = self.net(batch, uctr=self.monitor.uctr, ectr=self.monitor.ectr)
    self.loss_meter.update(out[&#39;loss&#39;], out[&#39;n_items&#39;])
    loss = out[&#39;loss&#39;] / out[&#39;n_items&#39;]

    # Add other losses if any
    if self.net.aux_loss:
        loss += sum(list(self.net.aux_loss.values()))

    # Backward pass
    loss.backward()

    # Update parameters (includes gradient clipping logic)
    self.optim.step()

    return time.time() - nn_start</code></pre>
</details>
</dd>
<dt id="pysimt.mainloop.MainLoop.train_epoch"><code class="name flex">
<span>def <span class="ident">train_epoch</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains a full epoch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_epoch(self):
    &#34;&#34;&#34;Trains a full epoch.&#34;&#34;&#34;
    self.print(&#39;Starting Epoch {}&#39;.format(self.monitor.ectr))

    nn_sec = 0.0
    eval_sec = 0.0
    total_sec = time.time()
    self.loss_meter.reset()
    self.oom_count = 0

    for batch in self.train_iterator:
        batch.device(self.dev_mgr.dev)
        self.monitor.uctr += 1

        try:
            nn_sec += self.train_batch(batch)
        except RuntimeError as e:
            if self.handle_oom and &#39;out of memory&#39; in e.args[0]:
                torch.cuda.empty_cache()
                self.oom_count += 1
            else:
                raise e

        if self.monitor.uctr % self.disp_freq == 0:
            # Send statistics
            self.tboard.log_scalar(
                &#39;train_LOSS&#39;, self.loss_meter.batch_loss, self.monitor.uctr)

            msg = &#34;Epoch {} - update {:10d} =&gt; loss: {:&gt;7.3f}&#34;.format(
                self.monitor.ectr, self.monitor.uctr,
                self.loss_meter.batch_loss)
            for key, value in self.net.aux_loss.items():
                val = value.item()
                msg += &#39; [{}: {:.3f}]&#39;.format(key, val)
                self.tboard.log_scalar(&#39;train_&#39; + key.upper(), val, self.monitor.uctr)
            msg += &#39; (#OOM: {})&#39;.format(self.oom_count)
            self.print(msg)

        # Do validation?
        if (not self.epoch_valid and
                self.monitor.ectr &gt;= self.eval_start and
                self.eval_freq &gt; 0 and
                self.monitor.uctr % self.eval_freq == 0):
            eval_start = time.time()
            self.do_validation()
            eval_sec += time.time() - eval_start

        if (self.checkpoint_freq and self.n_checkpoints &gt; 0 and
                self.monitor.uctr % self.checkpoint_freq == 0):
            self.print(&#39;Saving checkpoint...&#39;)
            self.monitor.save_checkpoint()

        # Check stopping conditions
        if self.monitor.early_bad == self.monitor.patience:
            self.print(&#34;Early stopped.&#34;)
            return False

        if self.monitor.uctr == self.max_iterations:
            self.print(&#34;Max iterations {} reached.&#34;.format(
                self.max_iterations))
            return False

    # All time spent for this epoch
    total_min = (time.time() - total_sec) / 60
    # All time spent during forward/backward/step
    nn_min = nn_sec / 60
    # All time spent during validation(s)
    eval_min = eval_sec / 60
    # Rest is iteration overhead + checkpoint saving
    overhead_min = total_min - nn_min - eval_min

    # Compute epoch loss
    epoch_loss = self.loss_meter.get()
    self.monitor.train_loss.append(epoch_loss)

    self.print(&#34;--&gt; Epoch {} finished with mean loss {:.5f}&#34;.format(
        self.monitor.ectr, epoch_loss))
    self.print(&#34;--&gt; Overhead/Training/Evaluation: {:.2f}/{:.2f}/{:.2f} &#34;
               &#34;mins (total: {:.2f} mins)   ({} samples/sec)&#34;.format(
                   overhead_min, nn_min, eval_min, total_min,
                   int(len(self.train_iterator.dataset) / nn_sec)))

    # Do validation?
    if self.epoch_valid and self.monitor.ectr &gt;= self.eval_start:
        self.do_validation()

    # Check whether maximum epoch is reached
    if self.monitor.ectr == self.max_epochs:
        self.print(&#34;Max epochs {} reached.&#34;.format(self.max_epochs))
        return False

    self.monitor.ectr += 1
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysimt" href="index.html">pysimt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysimt.mainloop.MainLoop" href="#pysimt.mainloop.MainLoop">MainLoop</a></code></h4>
<ul class="">
<li><code><a title="pysimt.mainloop.MainLoop.do_validation" href="#pysimt.mainloop.MainLoop.do_validation">do_validation</a></code></li>
<li><code><a title="pysimt.mainloop.MainLoop.train_batch" href="#pysimt.mainloop.MainLoop.train_batch">train_batch</a></code></li>
<li><code><a title="pysimt.mainloop.MainLoop.train_epoch" href="#pysimt.mainloop.MainLoop.train_epoch">train_epoch</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>